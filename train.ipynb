{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8428809,"sourceType":"datasetVersion","datasetId":5019236},{"sourceId":8428850,"sourceType":"datasetVersion","datasetId":5019268}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import random\nimport pandas as pd\nimport numpy as np\nimport os\nimport re\nimport glob\nimport cv2\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\nimport torchvision.models as models\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import classification_report\nfrom tqdm import tqdm\nimport warnings\nimport matplotlib.pyplot as plt\nfrom PIL import Image,ImageOps\nwarnings.filterwarnings(action='ignore') \ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-20T04:38:06.914503Z","iopub.execute_input":"2024-05-20T04:38:06.914873Z","iopub.status.idle":"2024-05-20T04:38:14.711733Z","shell.execute_reply.started":"2024-05-20T04:38:06.914841Z","shell.execute_reply":"2024-05-20T04:38:14.710738Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"CFG={\n    'IMG_SIZE':256,\n    'BATCH_SIZE':16,\n    'LEARNING_RATE':1e-5,\n    'SEED':42,\n    'EPOCHS':5\n}","metadata":{"execution":{"iopub.status.busy":"2024-05-20T04:38:15.755514Z","iopub.execute_input":"2024-05-20T04:38:15.755990Z","iopub.status.idle":"2024-05-20T04:38:15.760945Z","shell.execute_reply.started":"2024-05-20T04:38:15.755963Z","shell.execute_reply":"2024-05-20T04:38:15.759942Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(CFG['SEED']) ","metadata":{"execution":{"iopub.status.busy":"2024-05-20T04:38:16.523115Z","iopub.execute_input":"2024-05-20T04:38:16.523941Z","iopub.status.idle":"2024-05-20T04:38:16.531458Z","shell.execute_reply.started":"2024-05-20T04:38:16.523912Z","shell.execute_reply":"2024-05-20T04:38:16.530667Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"label_path='/kaggle/input/labels/label'\ntrain_label_path=os.path.join(label_path,'train')\nval_label_path=os.path.join(label_path,'val')\nlabel_json_list=['anger.json','happy.json','panic.json','sadness.json']","metadata":{"execution":{"iopub.status.busy":"2024-05-20T04:38:17.458705Z","iopub.execute_input":"2024-05-20T04:38:17.459054Z","iopub.status.idle":"2024-05-20T04:38:17.464002Z","shell.execute_reply.started":"2024-05-20T04:38:17.459027Z","shell.execute_reply":"2024-05-20T04:38:17.463066Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train_df=pd.DataFrame(columns=['img_path','gender','age','maxX','maxY','minX','minY','label'])\nval_df=pd.DataFrame(columns=['img_path','gender','age','maxX','maxY','minX','minY','label'])","metadata":{"execution":{"iopub.status.busy":"2024-05-20T04:42:12.864991Z","iopub.execute_input":"2024-05-20T04:42:12.865740Z","iopub.status.idle":"2024-05-20T04:42:12.873460Z","shell.execute_reply.started":"2024-05-20T04:42:12.865707Z","shell.execute_reply":"2024-05-20T04:42:12.872425Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"train_image_path_dict=dict()\nval_image_path_dict=dict()\nexp=['anger','happy','panic','sadness']","metadata":{"execution":{"iopub.status.busy":"2024-05-20T04:42:14.421446Z","iopub.execute_input":"2024-05-20T04:42:14.421782Z","iopub.status.idle":"2024-05-20T04:42:14.426946Z","shell.execute_reply.started":"2024-05-20T04:42:14.421757Z","shell.execute_reply":"2024-05-20T04:42:14.425904Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"import shutil\nextension=set()\nfor i in range(1,8):\n    path='/kaggle/input/images/img-00'+str(i)+'/img/'\n    if os.path.isdir(os.path.join(path,'train')):\n        train_path=os.path.join(path,'train')\n    if os.path.isdir(os.path.join(path,'val')):\n        val_path=os.path.join(path,'val')\n    for j in exp:\n        if os.path.isdir(os.path.join(train_path,j)):\n            train_exp_path=os.path.join(train_path,j)\n        if os.path.isdir(os.path.join(val_path,j)):\n            val_exp_path=os.path.join(val_path,j)\n        for j in [train_exp_path,val_exp_path]:\n            cate=j.split('/')[-2]\n            for f in os.listdir(j):\n                src=os.path.join(j,f)\n                extension.add(f.split('.')[-1])\n                if cate=='train':\n                    train_image_path_dict[f]=src\n                else:\n                    val_image_path_dict[f]=src","metadata":{"execution":{"iopub.status.busy":"2024-05-20T04:42:14.679774Z","iopub.execute_input":"2024-05-20T04:42:14.680087Z","iopub.status.idle":"2024-05-20T04:42:14.774479Z","shell.execute_reply.started":"2024-05-20T04:42:14.680060Z","shell.execute_reply":"2024-05-20T04:42:14.773699Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"import json\ndef make_df(dir_path,df,image_path_dict):\n    i=0\n    data_name=dir_path.split('/')[-1]\n    for label_name in label_json_list:\n        with open(os.path.join(dir_path,data_name+'_'+label_name),'r',encoding='cp949') as f:\n            file=json.load(f)\n        for v in file:\n            if v['filename'].split('.')[-1]=='jpeg':\n                continue\n            if v['gender']=='남':\n                gender=0\n            else:\n                gender=1\n            if v['faceExp_uploader']=='분노':\n                label='anger'\n            elif v['faceExp_uploader']=='기쁨':\n                label='happy'\n            elif v['faceExp_uploader']=='당황':\n                label='panic'\n            elif v['faceExp_uploader']=='슬픔':\n                label='sadness'\n            minX=(v['annot_A']['boxes']['minX']+v['annot_B']['boxes']['minX']+v['annot_C']['boxes']['minX'])/3\n            minY=(v['annot_A']['boxes']['minY']+v['annot_B']['boxes']['minY']+v['annot_C']['boxes']['minY'])/3\n            maxX=(v['annot_A']['boxes']['maxX']+v['annot_B']['boxes']['maxX']+v['annot_C']['boxes']['maxX'])/3\n            maxY=(v['annot_A']['boxes']['maxY']+v['annot_B']['boxes']['maxY']+v['annot_C']['boxes']['maxY'])/3\n            \n            df.loc[i]=[image_path_dict[v['filename']],gender,v['age'],int(maxX),int(maxY),int(minX),int(minY),label]\n            i+=1\n    return df.sample(frac=1).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T04:43:46.131535Z","iopub.execute_input":"2024-05-20T04:43:46.132530Z","iopub.status.idle":"2024-05-20T04:43:46.145763Z","shell.execute_reply.started":"2024-05-20T04:43:46.132495Z","shell.execute_reply":"2024-05-20T04:43:46.144486Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"train_df=make_df(train_label_path,train_df,train_image_path_dict)\nval_df=make_df(val_label_path,val_df,val_image_path_dict)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T04:43:48.098364Z","iopub.execute_input":"2024-05-20T04:43:48.098699Z","iopub.status.idle":"2024-05-20T04:43:57.209208Z","shell.execute_reply.started":"2024-05-20T04:43:48.098675Z","shell.execute_reply":"2024-05-20T04:43:57.208369Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\ntrain_df['label']=le.fit_transform(train_df['label'])\nval_df['label']=le.transform(val_df['label'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfig, axs = plt.subplots(nrows=1, ncols=5, figsize=(20, 20))\nfor i in range(5):\n    image_file = train_df.loc[i,'img_path']\n    minY, minX, maxX, maxY = map(int, [train_df.loc[i,'minY'], train_df.loc[i,'minX'], train_df.loc[i,'maxX'], train_df.loc[i,'maxY']])\n    img=cv2.imread(image_file)\n    img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n    img=img[minY:maxY,minX:maxX]\n    axs[i].imshow(img)\n    axs[i].axis('off')\n    axs[i].set_title(le.classes_[train_df.loc[i,'label']], size='large')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import timm\nmodel = timm.create_model('timm/swinv2_large_window12to16_192to256.ms_in22k_ft_in1k', pretrained=True,num_classes=1)\nmodel = torch.nn.DataParallel(model)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-20T05:01:44.671449Z","iopub.execute_input":"2024-05-20T05:01:44.672313Z","iopub.status.idle":"2024-05-20T05:01:48.702892Z","shell.execute_reply.started":"2024-05-20T05:01:44.672271Z","shell.execute_reply":"2024-05-20T05:01:48.702109Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"from torchvision.transforms import Compose, Resize, Normalize, ToTensor,RandomHorizontalFlip,RandomRotation,ColorJitter\nclass CustomDataset(Dataset):\n    def __init__(self, img_path, gender, maxX, maxY, minX, minY, transforms=None):\n        self.img_path = img_path\n        self.transforms = transforms\n        self.maxX = maxX\n        self.maxY = maxY\n        self.minX = minX\n        self.minY = minY\n        self.gender = gender\n        \n    def __getitem__(self, index):\n        image_path = self.img_path[index]\n        image = Image.open(image_path).convert(\"RGB\")\n        image = ImageOps.exif_transpose(image)\n        minY, minX, maxX, maxY = int(self.minY[index]), int(self.minX[index]), int(self.maxX[index]), int(self.maxY[index])\n        image = image.crop((minX, minY, maxX, maxY))\n        \n        if self.transforms is not None:\n            image = self.transforms(image)\n        if self.gender is not None:\n            gender=self.gender[index]\n            return image, gender\n        else:\n            return image\n        \n    def __len__(self):\n        return len(self.img_path)\n\ntrain_transform = Compose([\n    Resize((CFG['IMG_SIZE'], CFG['IMG_SIZE'])), \n    ToTensor(),\n    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n])\n\ntest_transform = Compose([\n    Resize((CFG['IMG_SIZE'],CFG['IMG_SIZE'])),\n    ToTensor(),\n    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n])\ntrain_ds=CustomDataset(train_df['img_path'].values,train_df['gender'].values,train_df['maxX'].values,train_df['maxY'].values,train_df['minX'].values,train_df['minY'].values,train_transform)\nval_ds=CustomDataset(val_df['img_path'].values,val_df['gender'].values,val_df['maxX'].values,val_df['maxY'].values,val_df['minX'].values,val_df['minY'].values,test_transform)\ntrain_loader=DataLoader(train_ds,batch_size=CFG['BATCH_SIZE'],shuffle=True,num_workers=0)\nval_loader=DataLoader(val_ds,batch_size=CFG['BATCH_SIZE'],shuffle=False,num_workers=0)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T04:52:43.402203Z","iopub.execute_input":"2024-05-20T04:52:43.402929Z","iopub.status.idle":"2024-05-20T04:52:43.417686Z","shell.execute_reply.started":"2024-05-20T04:52:43.402897Z","shell.execute_reply":"2024-05-20T04:52:43.416662Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"# 표정 학습","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom tqdm import tqdm\n\ndef train(model, optimizer, train_loader, val_loader, scheduler, device, early_stopping_patience=3):\n    model.to(device)\n    criterion = torch.nn.CrossEntropyLoss().to(device)\n    best_score = 0\n    best_model = None\n    patience = 0\n\n    for epoch in range(1, CFG['EPOCHS'] + 1):\n        model.train()\n        train_loss = []\n        for imgs, labels in tqdm(iter(train_loader)):\n            try:\n                imgs = imgs.float().to(device)\n                labels = labels.to(device)\n                optimizer.zero_grad()\n                output = model(imgs)\n                loss = criterion(output, labels)\n                loss.backward()\n                optimizer.step()\n                train_loss.append(loss.item())\n            except:\n                continue\n\n        _val_loss, _val_score = validation(model, criterion, val_loader, device)\n        _train_loss = np.mean(train_loss)\n        current_lr = optimizer.param_groups[0][\"lr\"]\n\n        print(f'Epoch [{epoch}], Train Loss: [{_train_loss:.5f}], Val Loss: [{_val_loss:.5f}], Val F1 Score: [{_val_score:.5f}], Learning Rate: {current_lr}')\n\n\n        if scheduler is not None:\n            scheduler.step()\n\n\n        if best_score < _val_score:\n            best_score = _val_score\n            best_model = model\n            patience = 0\n            torch.save(best_model, 'beitv2_large.pt')\n        else:\n            patience += 1\n\n\n        if patience >= early_stopping_patience:\n            print(f'Early stopping triggered at epoch {epoch}!')\n            break\n\n    return best_model\n\ndef validation(model, criterion, val_loader, device):\n    model.eval()\n    val_loss = []\n    preds, true_labels = [], []\n    with torch.no_grad():\n        for imgs, labels in tqdm(iter(val_loader)):\n            imgs = imgs.float().to(device)\n            labels = labels.to(device)\n            pred = model(imgs)\n            loss = criterion(pred, labels)\n            preds += pred.argmax(1).detach().cpu().numpy().tolist()\n            true_labels += labels.detach().cpu().numpy().tolist()\n            val_loss.append(loss.item())\n        _val_loss = np.mean(val_loss)\n        _val_score = f1_score(true_labels, preds, average='macro')\n    return _val_loss, _val_score\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 성별 학습","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\nfrom tqdm import tqdm\nfrom sklearn.metrics import accuracy_score\nimport copy  # deep copy를 위해 import\n\ndef train(model, optimizer, train_loader, val_loader, scheduler, device, early_stopping_patience=3):\n    criterion = nn.BCEWithLogitsLoss().to(device)\n    model.to(device)\n    best_model = None\n    best_loss = float('inf')\n    patience = 0\n\n    for epoch in range(1, CFG['EPOCHS'] + 1):\n        train_loss = []\n        train_preds, train_gender_true = [], []\n        model.train()\n\n        for img, gender in tqdm(iter(train_loader)):\n            img = img.float().to(device)\n            gender = gender.float().to(device)\n            optimizer.zero_grad()\n            output = model(img)\n\n            # 차원 맞추기\n            output = output.view(-1)\n            gender = gender.view(-1)\n\n            loss = criterion(output, gender)\n            loss.backward()\n            optimizer.step()\n\n            train_loss.append(loss.item())\n            train_preds += (torch.sigmoid(output) > 0.5).int().detach().cpu().numpy().tolist()\n            train_gender_true += gender.int().detach().cpu().numpy().tolist()\n\n        _train_loss = np.mean(train_loss)\n        _train_acc = accuracy_score(train_gender_true, train_preds)\n        _val_acc, _val_loss = validation(model, val_loader, criterion, device)\n        current_lr = optimizer.param_groups[0]['lr']\n\n        print(f'Epoch [{epoch}], Train Loss: [{_train_loss:.5f}], Train Acc: [{_train_acc:.5f}], '\n              f'Val Loss: [{_val_loss:.5f}], Val Acc: [{_val_acc:.5f}], Learning Rate: {current_lr}')\n\n        if scheduler is not None:\n            scheduler.step(_val_loss)\n\n        if best_loss > _val_loss:\n            best_loss = _val_loss\n            best_model = copy.deepcopy(model)\n            torch.save(best_model, 'swinv2_gender.pt')\n            patience = 0\n        else:\n            patience += 1\n\n        if patience >= early_stopping_patience:\n            print(f'Early stopping triggered at epoch {epoch}!')\n            break\n\n    return best_model\n\ndef validation(model, val_loader, criterion, device):\n    model.eval()\n    val_loss = []\n    val_preds, val_gender_true = [], []\n\n    with torch.no_grad():\n        for img, gender in tqdm(iter(val_loader)):\n            img = img.float().to(device)\n            gender = gender.float().to(device)\n            pred = model(img)\n\n            # 차원 맞추기\n            pred = pred.view(-1)\n            gender = gender.view(-1)\n\n            loss = criterion(pred, gender)\n\n            val_preds += (torch.sigmoid(pred) > 0.5).int().detach().cpu().numpy().tolist()\n            val_gender_true += gender.int().detach().cpu().numpy().tolist()\n            val_loss.append(loss.item())\n\n        _val_loss = np.mean(val_loss)\n        _val_acc = accuracy_score(val_gender_true, val_preds)\n\n    return _val_acc, _val_loss\n","metadata":{"execution":{"iopub.status.busy":"2024-05-20T06:13:57.329294Z","iopub.execute_input":"2024-05-20T06:13:57.329919Z","iopub.status.idle":"2024-05-20T06:13:57.349943Z","shell.execute_reply.started":"2024-05-20T06:13:57.329888Z","shell.execute_reply":"2024-05-20T06:13:57.349043Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"markdown","source":"# 나이 학습","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\nfrom tqdm import tqdm\nfrom sklearn.metrics import mean_absolute_error\nimport copy\n\ndef train(model, optimizer, train_loader, val_loader, scheduler, device, early_stopping_patience=3):\n    criterion = nn.MSELoss().to(device)\n    model.to(device)\n    best_model = None\n    best_loss = float('inf')\n    patience = 0\n\n    for epoch in range(1,CFG['EPOCHS']+1):\n        train_loss = []\n        train_preds, train_age_true = [], []\n        model.train()\n\n        for img, age in tqdm(iter(train_loader)):\n            img = img.float().to(device)\n            age = age.float().to(device)\n            optimizer.zero_grad()\n            output = model(img)\n            loss = criterion(output, age) \n            loss.backward()\n            optimizer.step()\n\n            train_loss.append(loss.item())\n            train_preds += output.detach().cpu().numpy().tolist()  \n            train_age_true += age.detach().cpu().numpy().tolist()\n        _train_loss=np.mean(train_loss)\n        _train_mae = mean_absolute_error(train_age_true, train_preds)\n        _val_mae, _val_loss= validation(model, val_loader, criterion, device)\n        current_lr = optimizer.param_groups[0]['lr']\n\n        print(f'Epoch [{epoch}], Train MAE: [{_train_mae:.5f}], Train Loss: [{_train_loss:.5f}], Val MAE: [{_val_mae:.5f}], Val Loss: [{_val_loss:.5f}], Learning Rate: {current_lr}')\n\n        if scheduler is not None:\n            scheduler.step()\n\n        if best_loss > _val_mae:\n            best_loss = _val_mae\n            best_model = copy.deepcopy(model)\n            torch.save(best_model, 'swinv2_age.pt')\n            patience = 0\n        else:\n            patience += 1\n\n        if patience >= early_stopping_patience:\n            print(f'Early stopping triggered at epoch {epoch}!')\n            break\n\n    return best_model\n\ndef validation(model, val_loader, criterion, device):\n    model.eval()\n    val_loss = []\n    val_preds, val_age_true = [], []\n\n    with torch.no_grad():\n        for img, age in tqdm(iter(val_loader)):\n            img = img.float().to(device)\n            age = age.float().to(device)\n            pred = model(img)\n            loss = criterion(pred, age)  # pred를 squeeze하여 차원을 맞춤\n\n            val_preds += pred.detach().cpu().numpy().tolist()  # flatten을 사용하여 1차원 배열로 변환\n            val_age_true += age.detach().cpu().numpy().tolist()\n            val_loss.append(loss.item())\n        _val_loss=np.mean(val_loss)\n        _val_mae = mean_absolute_error(val_age_true, val_preds)\n\n    return _val_mae, _val_loss\n","metadata":{"execution":{"iopub.status.busy":"2024-05-20T05:02:03.705949Z","iopub.execute_input":"2024-05-20T05:02:03.706820Z","iopub.status.idle":"2024-05-20T05:02:03.724566Z","shell.execute_reply.started":"2024-05-20T05:02:03.706785Z","shell.execute_reply":"2024-05-20T05:02:03.723578Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"model.eval()\noptimizer=torch.optim.AdamW(params=model.parameters(),lr=CFG['LEARNING_RATE'])\nscheduler=torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=10)\ninfer_model=train(model,optimizer,train_loader,val_loader,scheduler,device)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T06:14:00.459346Z","iopub.execute_input":"2024-05-20T06:14:00.459815Z","iopub.status.idle":"2024-05-20T07:51:54.347122Z","shell.execute_reply.started":"2024-05-20T06:14:00.459782Z","shell.execute_reply":"2024-05-20T07:51:54.346024Z"},"trusted":true},"execution_count":69,"outputs":[{"name":"stderr","text":"100%|██████████| 369/369 [17:20<00:00,  2.82s/it]\n100%|██████████| 73/73 [02:12<00:00,  1.82s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1], Train Loss: [0.01439], Train Acc: [0.99575], Val Loss: [0.02683], Val Acc: [0.99057], Learning Rate: 1e-05\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 369/369 [17:25<00:00,  2.83s/it]\n100%|██████████| 73/73 [02:08<00:00,  1.76s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2], Train Loss: [0.00439], Train Acc: [0.99830], Val Loss: [0.13300], Val Acc: [0.97942], Learning Rate: 9.999289656187745e-06\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 369/369 [17:24<00:00,  2.83s/it]\n100%|██████████| 73/73 [02:17<00:00,  1.89s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3], Train Loss: [0.00844], Train Acc: [0.99796], Val Loss: [0.01715], Val Acc: [0.99400], Learning Rate: 9.98255114040809e-06\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 369/369 [17:27<00:00,  2.84s/it]\n100%|██████████| 73/73 [02:09<00:00,  1.77s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [4], Train Loss: [0.00777], Train Acc: [0.99864], Val Loss: [0.03346], Val Acc: [0.99400], Learning Rate: 9.999709810960851e-06\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 369/369 [17:12<00:00,  2.80s/it]\n100%|██████████| 73/73 [02:10<00:00,  1.79s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch [5], Train Loss: [0.00069], Train Acc: [0.99983], Val Loss: [0.05577], Val Acc: [0.98542], Learning Rate: 9.998894880263873e-06\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]}]}