{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install timm\n!pip install mediapipe","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import mediapipe as mp\nfrom PIL import Image, ImageOps\nimport numpy as np\n\nmp_face_detection = mp.solutions.face_detection\nmp_drawing = mp.solutions.drawing_utils\n\ndef detect_and_crop_face(image):\n    with mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.5) as face_detection:\n        image_np = np.array(image)\n        results = face_detection.process(image_np)\n        if results.detections:\n            detection = results.detections[0]\n            bbox = detection.location_data.relative_bounding_box\n            ih, iw, _ = image_np.shape\n            xmin = int(bbox.xmin * iw)\n            ymin = int(bbox.ymin * ih)\n            width = int(bbox.width * iw)\n            height = int(bbox.height * ih)\n            xmax = xmin + width\n            ymax = ymin + height\n            face = image.crop((xmin, ymin, xmax, ymax))\n            return face\n        else:\n            return -1\n\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\npath='/content/drive/MyDrive/TEST_DATA_SET'\nanger_path=os.path.join(path,'anger')\nhappy_path=os.path.join(path,'happy')\npanic_path=os.path.join(path,'panic')\nsadness_path=os.path.join(path,'sadness')\nlabel_path=os.path.join(path,'label (라벨링)')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\ntest_df=pd.DataFrame(columns=['img_path','label','gender','age'])\ni=0\nfor label_name in os.listdir(label_path):\n    with open(os.path.join(label_path,label_name),'r',encoding='cp949') as f:\n        file=json.load(f)\n        for v in file:\n            if v['filename'].split('.')[-1]=='jpeg':\n                continue\n            if v['gender']=='남':\n                gender='남자'\n            else:\n                gender='여자'\n            if v['faceExp_uploader']=='분노':\n                label='anger'\n            elif v['faceExp_uploader']=='기쁨':\n                label='happy'\n            elif v['faceExp_uploader']=='당황':\n                label='panic'\n            elif v['faceExp_uploader']=='슬픔':\n                label='sadness'\n            test_df.loc[i]=[os.path.join(path,label,v['filename']),label,gender,v['age']]\n            i+=1\ntest_df.sample(frac=1).reset_index(drop=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision.transforms import Compose, Resize, Normalize, ToTensor\nexpression_test_transform = Compose([\n    Resize((224, 224)),\n    ToTensor(),\n    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n])\nage_test_transform=Compose([\n    Resize((256,256)),\n    ToTensor(),\n    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n])\ngender_test_transform=Compose([\n    Resize((256,256)),\n    ToTensor(),\n    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"expression_model = torch.load(\"/content/drive/MyDrive/best_model.pt\", map_location=device)\nage_model = torch.load('/content/drive/MyDrive/swinv2_ages.pt', map_location=device)\ngender_model = torch.load('/content/drive/MyDrive/swinv2_gender.pt', map_location=device)\nexpression_model.eval()\nage_model.eval()\ngender_model.eval()\nexpression_model.to(device)\nage_model.to(device)\ngender_model.to(device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, mean_absolute_error\nlabel=['anger','happy','panic','sadness']\ngender=['남자','여자']\nexpression_preds,expression_true_labels=[],[]\nage_preds,age_true_labels=[],[]\ngender_preds,gender_true_labels=[],[]\nwith torch.no_grad():\n    for i in tqdm(range(len(test_df))):\n        img_path=test_df.iloc[i]['img_path']\n        img=Image.open(img_path).convert('RGB')\n        img=ImageOps.exif_transpose(img)\n        cropped_img=detect_and_crop_face(img)\n        if cropped_img==-1:\n            print(test_df.iloc[i]['img_path'])\n            continue\n        expression_img=expression_test_transform(cropped_img)\n        age_img=age_test_transform(cropped_img)\n        gender_img=gender_test_transform(cropped_img)\n        expression_img=expression_img.unsqueeze(0).to(device)\n        age_img=age_img.unsqueeze(0).to(device)\n        gender_img=gender_img.unsqueeze(0).to(device)\n        expression_pred=expression_model(expression_img)\n        age_pred=age_model(age_img)\n        gender_pred=gender_model(gender_img)\n        expression_pred=label[expression_pred.argmax(1).detach().cpu().numpy().tolist()[0]]\n        age_pred=age_pred.detach().cpu().item()\n        gender_pred=gender[(torch.sigmoid(gender_pred) > 0.5).int().detach().cpu().numpy().tolist()[0][0]]\n        expression_preds.append(expression_pred)\n        expression_true_labels.append(test_df.iloc[i]['label'])\n        age_preds.append(age_pred)\n        age_true_labels.append(test_df.iloc[i]['age'])\n        gender_preds.append(gender_pred)\n        gender_true_labels.append(test_df.iloc[i]['gender'])\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"expression_acc=accuracy_score(expression_true_labels,expression_preds)\nage_mae = mean_absolute_error(age_true_labels, age_preds)\ngender_acc=accuracy_score(gender_true_labels,gender_preds)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'expression_acc:{expression_acc}')\nprint(f'age_mae:{age_mae}')\nprint(f'gender_acc:{gender_acc}')","metadata":{},"execution_count":null,"outputs":[]}]}